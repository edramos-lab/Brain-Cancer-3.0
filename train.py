# -*- coding: utf-8 -*-
"""Copy of Entrenamiento efficientnet_b0

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uVXWJDy6w3VNp215Dnmrll9wI1Q9kKui
"""



## Permisos para acceder a la ubicación
'''from google.colab import drive
drive.mount('/content/drive')'''

!git clone https://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@github.com/edramos-lab/Cancer-Brain-3.0.git

# prompt: unzip /content/Cancer-Brain-3.0/BRATS_2019_Dataset.zip

!unzip /content/Cancer-Brain-3.0/BRATS_2019_Dataset.zip

import os
import numpy as np
import nibabel as nib
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
import timm
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import random
import  sklearn.metrics
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
import wandb
from torch.optim.lr_scheduler import CosineAnnealingLR


# Definir el dispositivo
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Ruta del conjunto de datos
#output_dir='/content/drive/MyDrive/BRATS_2019_Dataset'
output_dir='/content/BRATS_2019_Dataset'
# Crear directorio de salida si no existe
os.makedirs(output_dir, exist_ok=True)

def visualize_images(class_name, num_images=4):
    class_path = os.path.join(output_dir, class_name)
    images = [img for img in os.listdir(class_path) if img.endswith('.jpg')]

    plt.figure(figsize=(10, 5))
    for i in range(min(num_images, len(images))):
        img_path = os.path.join(class_path, images[i])
        img = Image.open(img_path)
        plt.subplot(1, num_images, i + 1)
        plt.imshow(img, cmap='gray')
        plt.axis('off')
        plt.title(images[i])

    plt.show()

# Visualizar algunas imágenes de HGG y LGG
#visualize_images('/content/drive/MyDrive/BRATS_2019_Dataset/training/HGG_training', num_images=4)
#visualize_images('/content/drive/MyDrive/BRATS_2019_Dataset/training/LGG_training', num_images=4)
visualize_images('/content/BRATS_2019_Dataset/training/HGG_training', num_images=4)
visualize_images('/content/BRATS_2019_Dataset/training/LGG_training', num_images=4)

class CustomDataset(Dataset):
    def __init__(self, hgg_dir, lgg_dir, transform=None):
        """
        Inicializa el dataset.

        :param hgg_dir: Directorio de imágenes de la clase HGG.
        :param lgg_dir: Directorio de imágenes de la clase LGG.
        :param transform: Transformaciones a aplicar a las imágenes.
        """
        self.hgg_dir = hgg_dir
        self.lgg_dir = lgg_dir
        self.transform = transform

        # Listas para almacenar las rutas de las imágenes y las etiquetas correspondientes
        self.images = []
        self.labels = []

        # Cargar imágenes y etiquetas
        self._load_images()

    def _load_images(self):
        """
        Carga las imágenes de los directorios HGG y LGG, y asigna etiquetas.
        """
        # Cargar imágenes de HGG
        for img_name in os.listdir(self.hgg_dir):
            img_path = os.path.join(self.hgg_dir, img_name)
            self.images.append(img_path)
            self.labels.append(1)  # Etiqueta 1 para HGG

        # Cargar imágenes de LGG
        for img_name in os.listdir(self.lgg_dir):
            img_path = os.path.join(self.lgg_dir, img_name)
            self.images.append(img_path)
            self.labels.append(0)  # Etiqueta 0 para LGG

    def __len__(self):
        """
        Devuelve el número total de imágenes en el dataset.
        """
        return len(self.images)

    def __getitem__(self, idx):
        """
        Devuelve una imagen y su etiqueta correspondiente.

        :param idx: Índice de la imagen que se desea obtener.
        :return: Tupla (imagen, etiqueta).
        """
        # Obtener la ruta de la imagen y cargarla
        img_path = self.images[idx]
        img = Image.open(img_path).convert("RGB")  # Asegurarse de que la imagen sea RGB

        # Obtener la etiqueta correspondiente
        label = self.labels[idx]

        # Aplicar la transformación si está definida
        if self.transform:
            img = self.transform(img)

        return img, label

class CustomTestDataset(Dataset):
    def __init__(self, hgg_test_dir, lgg_test_dir, num_images_per_class=200, transform=None):
        """
        Inicializa el dataset de prueba.

        :param hgg_test_dir: Directorio de imágenes de la clase HGG para prueba.
        :param lgg_test_dir: Directorio de imágenes de la clase LGG para prueba.
        :param num_images_per_class: Número de imágenes a tomar de cada clase (HGG y LGG).
        :param transform: Transformaciones a aplicar a las imágenes.
        """
        self.images = []
        self.labels = []
        self.transform = transform

        # Cargar imágenes de la clase HGG
        hgg_images = os.listdir(hgg_test_dir)[:num_images_per_class]
        for img in hgg_images:
            self.images.append(os.path.join(hgg_test_dir, img))
            self.labels.append(1)  # HGG

        # Cargar imágenes de la clase LGG
        lgg_images = os.listdir(lgg_test_dir)[:num_images_per_class]
        for img in lgg_images:
            self.images.append(os.path.join(lgg_test_dir, img))
            self.labels.append(0)  # LGG

    def __len__(self):
        """
        Devuelve el número total de imágenes en el dataset.
        """
        return len(self.images)

    def __getitem__(self, idx):
        """
        Devuelve una imagen y su etiqueta correspondiente.

        :param idx: Índice de la imagen que se desea obtener.
        :return: Tupla (imagen, etiqueta).
        """
        # Cargar la imagen
        img = Image.open(self.images[idx]).convert("RGB")  # Asegurarse de que sea RGB

        # Aplicar las transformaciones si están definidas
        if self.transform:
            img = self.transform(img)

        # Devolver la imagen y la etiqueta
        return img, self.labels[idx]

##Verificar numero de imagenes en training folder
#hgg_dir = '/content/drive/MyDrive/BRATS_2019_Dataset/training/HGG_training'
#lgg_dir = '/content/drive/MyDrive/BRATS_2019_Dataset/training/LGG_training'
hgg_dir = '/content/BRATS_2019_Dataset/training/HGG_training'
lgg_dir = '/content/BRATS_2019_Dataset/training/LGG_training'

# Contar imágenes en HGG
hgg_images = os.listdir(hgg_dir)
hgg_count = len(hgg_images)

# Contar imágenes en LGG
lgg_images = os.listdir(lgg_dir)
lgg_count = len(lgg_images)

# Imprimir resultados
print(f'Número de imágenes en HGG: {hgg_count}')
print(f'Número de imágenes en LGG: {lgg_count}')

# Definir transformaciones para el conjunto de training
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Ajusta el tamaño
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Cambiar brillo, contraste, etc.
    transforms.RandomRotation(30),  # Rotación aleatoria
    transforms.RandomHorizontalFlip(),    #Se voltea la imagen horizontalmente
    transforms.RandomVerticalFlip(),   #Se voltea verticalmente
   # transforms.RandomPerspective(),
    transforms.ToTensor()
    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización de ImageNet
])

#Definir transformaciones para el conjunto de testing
# Transformaciones para el conjunto de prueba
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalización de ImageNet
])

# Cargar el conjunto de datos completo
train_dataset = CustomDataset(hgg_dir, lgg_dir, transform=train_transform)

##Cargar el conjunto de prueba
hgg_test_dir = '/content/BRATS_2019_Dataset/testing/HGG_test' # Cambia a tu ruta
lgg_test_dir = '/content/BRATS_2019_Dataset/testing/LGG_test'  # Cambia a tu ruta
num_images = 200  # Ajusta el número de imágenes por clase

test_dataset = CustomTestDataset(hgg_test_dir, lgg_test_dir, num_images_per_class=num_images, transform=test_transform)

# Número de imágenes a mostrar
num_images_to_show = 10

# Crear la figura con subgráficos en una fila
fig, axes = plt.subplots(1, num_images_to_show, figsize=(10, 5))

# Mostrar algunas imágenes transformadas
for i in range(num_images_to_show):
    # Seleccionar una imagen aleatoria y su etiqueta
    idx = torch.randint(0, len(train_dataset), (1,)).item()  # Generar un índice aleatorio
    image, label = train_dataset[idx]  # Obtener la imagen y la etiqueta

    # Convertir la imagen de tensor (C, H, W) a (H, W, C) para la visualización
    image = image.permute(1, 2, 0).numpy()  # Convertir a formato (H, W, C)

    # Mostrar la imagen en el subplot correspondiente
    ax = axes[i]  # Obtener el eje para esta imagen
    ax.imshow(image)
    ax.set_title(f'Label: {label}')
    ax.axis('off')  # Desactivar los ejes

# Ajustar el layout para que no se solapen las imágenes
plt.tight_layout()
plt.show()

import torch
from torch.utils.data import random_split

# Calcular el número de muestras del 10% para el conjunto de entrenamiento
num_train_samples = int(len(train_dataset) * 0.30)

# Para el conjunto de prueba, asegurarte de tener al menos un número mínimo de muestras
min_test_samples = 20  # Puedes ajustar este número según lo que necesites
num_test_samples = max(int(len(test_dataset) * 0.30), min_test_samples)

# Dividir aleatoriamente el dataset de entrenamiento
train_subset, _ = random_split(train_dataset, [num_train_samples, len(train_dataset) - num_train_samples])

# Dividir aleatoriamente el dataset de prueba
test_subset, _ = random_split(test_dataset, [num_test_samples, len(test_dataset) - num_test_samples])

# Verificar los tamaños de los subconjuntos
print(f"Tamaño del conjunto de entrenamiento (train_subset): {len(train_subset)}")
print(f"Tamaño del conjunto de prueba (test_subset): {len(test_subset)}")

# Crear los DataLoaders
train_loader = DataLoader(train_subset, batch_size= 32, shuffle=True)
test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)

# Verificar el tamaño de los DataLoaders
print(f'Tamaño de los batches en el DataLoader de entrenamiento: {len(train_loader)}')
print(f'Tamaño de los batches en el DataLoader de prueba: {len(test_loader)}')

# Comprobar si hay una GPU disponible y usarla
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Definir el modelo

model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=2)

model = model.to(device)   ### inicializa con pesos preentrenados en ImageNet+

# Definir el optimizador
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Ajusta la tasa de aprendizaje aquí  lr=1e-4

# Definir la función de pérdida
criterion = nn.CrossEntropyLoss()
  # Para clasificación binaria

# Imprimir la configuración
print(f"Función de pérdida: {criterion}")
print(f"Optimizador: {optimizer}")

import torch
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import KFold
import wandb

# Definir el número de pliegues
num_folds = 3
kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

# Ciclo de K-Fold
for fold, (train_idx, val_idx) in enumerate(kf.split(train_subset)):
    # Inicializa WandB para cada pliegue
    wandb.init(project="EfficientNet_B0")

    # Configurar los datos de entrenamiento y validación para este pliegue
    train_fold_subset = torch.utils.data.Subset(train_subset, train_idx)
    val_fold_subset = torch.utils.data.Subset(train_subset, val_idx)

    # Crear los DataLoaders con estos subconjuntos
    train_loader = DataLoader(train_fold_subset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_fold_subset, batch_size=32, shuffle=False)

    # Reiniciar el modelo y optimizador para cada pliegue
    #odel = model.to(device)  # Asume que ya tienes el dispositivo
    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Ajusta el learning rate inicial

    # Usamos el ReduceLROnPlateau
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=False)

    # Ciclo de entrenamiento
    num_epochs = 10  # Ajusta según sea necesario
    train_confusion_matrices = []  # Para almacenar matrices de confusión del entrenamiento
    test_confusion_matrices = []  # Para almacenar matrices de confusión del test

    for epoch in range(num_epochs):
        model.train()  # Poner el modelo en modo de entrenamiento
        running_loss = 0.0
        all_labels = []
        all_outputs = []
        all_probs = []  # Para almacenar probabilidades para AUC

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)

            # Calcular la pérdida
            loss = criterion(outputs, labels.long())
            loss.backward()
            optimizer.step()

            running_loss += loss.item()  # Almacenar la pérdida

            # Guardar etiquetas, predicciones y probabilidades
            all_labels.extend(labels.cpu().numpy())
            pred = torch.argmax(outputs, dim=1).detach().cpu().numpy()
            all_outputs.extend(pred)

            # Almacenar probabilidades para calcular AUC
            probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()  # Probabilidades de la clase positiva
            all_probs.extend(probs)

        # Calcular métricas para el conjunto de entrenamiento
        train_accuracy = accuracy_score(all_labels, all_outputs)
        train_precision = precision_score(all_labels, all_outputs, average='binary')
        train_recall = recall_score(all_labels, all_outputs, average='binary')
        train_f1 = f1_score(all_labels, all_outputs, average='binary')
        train_cm = confusion_matrix(all_labels, all_outputs)
        train_confusion_matrices.append(train_cm)  # Almacenar la matriz de confusión

        # Log de métricas a WandB
        wandb.log({
            "Fold": fold + 1,
            "Epoch": epoch + 1,
            "Loss": running_loss / len(train_loader),
            "Accuracy": train_accuracy,
            "Precision": train_precision,
            "Recall": train_recall,
            "F1 Score": train_f1,
            "Learning Rate": optimizer.param_groups[0]['lr'],
            "Model": model
        })

        # Mostrar resultados de entrenamiento
        print(f"Fold [{fold + 1}], Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, "
              f"Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, "
              f"Recall: {train_recall:.4f}, F1 Score: {train_f1:.4f}")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']}")

        # Validación al final de cada época
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            all_val_labels = []
            all_val_outputs = []
            all_val_probs = []  # Para almacenar probabilidades para AUC

            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels.long())
                val_loss += loss.item()

                all_val_labels.extend(labels.cpu().numpy())
                val_pred = torch.argmax(outputs, dim=1).detach().cpu().numpy()
                all_val_outputs.extend(val_pred)

                # Almacenar probabilidades para calcular AUC
                val_probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()
                all_val_probs.extend(val_probs)

            # Calcular métricas para el conjunto de validación
            val_accuracy = accuracy_score(all_val_labels, all_val_outputs)
            val_precision = precision_score(all_val_labels, all_val_outputs, average='binary')
            val_recall = recall_score(all_val_labels, all_val_outputs, average='binary')
            val_f1 = f1_score(all_val_labels, all_val_outputs, average='binary')

            # Log de métricas de validación a WandB
            wandb.log({
                "Fold": fold + 1,
                "Validation Loss": val_loss / len(val_loader),
                "Validation Accuracy": val_accuracy,
                "Validation Precision": val_precision,
                "Validation Recall": val_recall,
                "Validation F1 Score": val_f1,
            })

            # Mostrar resultados de validación
            print(f"Fold [{fold + 1}], Validation Loss: {val_loss / len(val_loader):.4f}, "
                  f"Validation Accuracy: {val_accuracy:.4f}, Validation Precision: {val_precision:.4f}, "
                  f"Validation Recall: {val_recall:.4f}, Validation F1 Score: {val_f1:.4f}")

        # Ajustar el learning rate con ReduceLROnPlateau basado en la validación loss
        scheduler.step(val_loss)

        # testing
        model.eval()
        with torch.no_grad():
            all_test_labels = []
            all_test_outputs = []
            all_test_probs = []  # Para almacenar probabilidades para AUC
            test_loss = 0.0

            for images, labels in test_loader:    # Usar test_loader aquí
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels.long())
                test_loss += loss.item()

                all_test_labels.extend(labels.cpu().numpy())
                test_pred = torch.argmax(outputs, dim=1).detach().cpu().numpy()
                all_test_outputs.extend(test_pred)

                # Almacenar probabilidades para calcular AUC
                test_probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()  # Probabilidades de la clase positiva
                all_test_probs.extend(test_probs)

            # Calcular métricas para el conjunto de prueba
            test_accuracy = accuracy_score(all_test_labels, all_test_outputs)
            test_precision = precision_score(all_test_labels, all_test_outputs, average='binary')
            test_recall = recall_score(all_test_labels, all_test_outputs, average='binary')
            test_f1 = f1_score(all_test_labels, all_test_outputs, average='binary')
            test_cm = confusion_matrix(all_test_labels, all_test_outputs)
            test_confusion_matrices.append(test_cm)  # Almacenar la matriz de confusión

            # Log de métricas de prueba a WandB
            wandb.log({
                "Fold": fold + 1,
                "Test Loss": test_loss / len(test_loader),
                "Test Accuracy": test_accuracy,
                "Test Precision": test_precision,
                "Test Recall": test_recall,
                "Test F1 Score": test_f1,
                "Model": model
            })

            # Mostrar resultados de prueba
            print(f"Fold [{fold + 1}], Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.4f}, "
                  f"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, "
                  f"Test F1 Score: {test_f1:.4f}")

    # Graficar las matrices de confusión individuales para cada pliegue
    plt.figure(figsize=(6, 6))
    sns.heatmap(test_confusion_matrices[-1], annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=['Low Grade', 'High Grade'], yticklabels=['Low Grade', 'High Grade'])
    plt.title(f'Test Confusion Matrix for Fold {fold + 1}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    wandb.log({"Test Confusion Matrix": wandb.Image(plt)})

    plt.tight_layout()
    plt.show()

    # Finaliza el registro en WandB para este pliegue
    wandb.finish()